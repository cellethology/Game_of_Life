"""
ğŸ“Š æ··æ·†çŸ©é˜µå¯è§†åŒ–
4å¼ å›¾: 3Ã—3/5Ã—5 patch Ã— æ— æƒé‡/æœ‰æƒé‡
æ¯å¼ å›¾æ˜¾ç¤º3Ã—3ç½‘æ ¼: 3å¯†åº¦ Ã— 3æ—¶é—´ç‚¹çš„æ··æ·†çŸ©é˜µ
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd

def load_results():
    """åŠ è½½å®éªŒç»“æœ"""
    # æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶
    result_files = list(Path('.').glob('results_18_settings_*.json'))
    if not result_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶!")
        return None

    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ“‚ åŠ è½½ç»“æœæ–‡ä»¶: {latest_file}")

    with open(latest_file, 'r') as f:
        results = json.load(f)

    return results

def aggregate_results_by_setting(results):
    """æŒ‰è®¾ç½®èšåˆç»“æœ"""
    # è®¾ç½®ç»“æ„: {density}_{time_point}_{patch_size}_{seed}_{weight_type}
    aggregated = {}

    for setting_name, result_list in results.items():
        # è§£æè®¾ç½®åç§°
        parts = setting_name.split('_')
        density = parts[0]  # p0.2, p0.4, p0.6
        time_point = parts[1]  # early, mid, late
        patch_size = parts[2]  # patch3, patch5

        # æŒ‰è®¾ç½®åˆ†ç»„
        setting_key = f"{density}_{time_point}"
        if patch_size not in aggregated:
            aggregated[patch_size] = {}

        if setting_key not in aggregated[patch_size]:
            aggregated[patch_size][setting_key] = {'unweighted': [], 'weighted': []}

        # æŒ‰æƒé‡ç±»å‹åˆ†ç»„
        for result in result_list:
            if result['use_weight']:
                aggregated[patch_size][setting_key]['weighted'].append(result)
            else:
                aggregated[patch_size][setting_key]['unweighted'].append(result)

    return aggregated

def compute_confusion_matrix_matrix(results_list):
    """è®¡ç®—èšåˆçš„æ··æ·†çŸ©é˜µ"""
    if not results_list:
        return np.array([[0, 0], [0, 0]])

    total_tn = sum(r['tn'] for r in results_list)
    total_fp = sum(r['fp'] for r in results_list)
    total_fn = sum(r['fn'] for r in results_list)
    total_tp = sum(r['tp'] for r in results_list)

    return np.array([[total_tn, total_fp], [total_fn, total_tp]])

def compute_normalized_confusion_matrix(cm):
    """è®¡ç®—å½’ä¸€åŒ–çš„æ··æ·†çŸ©é˜µ"""
    return cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

def create_confusion_matrix_plot(aggregated_results, patch_size, use_weight):
    """åˆ›å»ºå•ä¸ªæ··æ·†çŸ©é˜µå›¾ (3Ã—3ç½‘æ ¼)"""

    # å¯†åº¦å’Œæ—¶é—´ç‚¹é¡ºåº
    densities = ['p0.2', 'p0.4', 'p0.6']
    time_points = ['early', 'mid', 'late']

    # åˆ›å»º3Ã—3å­å›¾
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    fig.suptitle(f'{patch_size}Ã—{patch_size} Patch - {"Weighted" if use_weight else "Unweighted"}',
                 fontsize=16, fontweight='bold')

    patch_key = f"patch{patch_size}"
    weight_type = "weighted" if use_weight else "unweighted"

    # ä¸ºæ¯ä¸ªå­å›¾åˆ›å»ºæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    for i, density in enumerate(densities):
        for j, time_point in enumerate(time_points):
            ax = axes[i, j]

            setting_key = f"{density}_{time_point}"

            if (patch_key in aggregated_results and
                setting_key in aggregated_results[patch_key] and
                aggregated_results[patch_key][setting_key][weight_type]):

                results_list = aggregated_results[patch_key][setting_key][weight_type]

                # è®¡ç®—èšåˆæ··æ·†çŸ©é˜µ
                cm = compute_confusion_matrix_matrix(results_list)

                # è®¡ç®—æŒ‡æ ‡
                total = cm.sum()
                accuracy = (cm[0,0] + cm[1,1]) / total
                precision = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
                recall = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

                # è·å–æƒé‡ä¿¡æ¯
                avg_weight = np.mean([r['pos_weight'] for r in results_list]) if use_weight else 1.0

                # åˆ›å»ºçƒ­åŠ›å›¾ (ä½¿ç”¨å½’ä¸€åŒ–å€¼)
                cm_normalized = compute_normalized_confusion_matrix(cm)

                sns.heatmap(cm_normalized, annot=cm, fmt='d', cmap='Blues',
                           ax=ax, cbar=False, vmin=0, vmax=1,
                           annot_kws={'size': 10, 'weight': 'bold'})

                # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
                title = f"{density.upper()} {time_point.upper()}\n"
                title += f"Acc: {accuracy:.3f}, Prec: {precision:.3f}, Rec: {recall:.3f}\n"
                if use_weight:
                    title += f"Weight: {avg_weight:.3f}"
                else:
                    title += f"Weight: 1.000"

                ax.set_title(title, fontsize=10, fontweight='bold')
                ax.set_xlabel('Predicted', fontsize=9)
                ax.set_ylabel('Actual', fontsize=9)

                # è®¾ç½®åˆ»åº¦æ ‡ç­¾
                ax.set_xticklabels(['0 (Dead)', '1 (Alive)'], fontsize=8)
                ax.set_yticklabels(['0 (Dead)', '1 (Alive)'], fontsize=8)

            else:
                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ˜¾ç¤ºç©ºç™½
                ax.text(0.5, 0.5, 'No Data', ha='center', va='center',
                        transform=ax.transAxes, fontsize=12)
                ax.set_title(f"{density.upper()} {time_point.upper()}", fontsize=10)
                ax.set_xlabel('Predicted', fontsize=9)
                ax.set_ylabel('Actual', fontsize=9)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout(rect=[0, 0.08, 1, 0.95])

    # æ·»åŠ å…¨å±€é¢œè‰²æ¡
    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
    sm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=0, vmax=1))
    sm.set_array([])
    fig.colorbar(sm, cax=cbar_ax, label='Normalized Rate')

    # æ·»åŠ å›¾ä¾‹è¯´æ˜
    fig.text(0.5, 0.02,
             'Matrix Format: [[TN, FP], [FN, TP]] | Values: Absolute Counts | Colors: Normalized Rates\n'
             'Densities: p0.2 (Low), p0.4 (Medium), p0.6 (High) | Time: early, mid, late burn-in steps',
             ha='center', va='center', fontsize=10, style='italic')

    return fig

def create_summary_statistics(aggregated_results):
    """åˆ›å»ºæ±‡æ€»ç»Ÿè®¡å›¾"""

    densities = ['p0.2', 'p0.4', 'p0.6']
    time_points = ['early', 'mid', 'late']

    # åˆ›å»ºæŒ‡æ ‡å¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Weight vs Unweighted Performance Comparison', fontsize=16, fontweight='bold')

    metrics = ['accuracy', 'precision', 'recall', 'f1']
    metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score']

    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        ax = axes[idx // 2, idx % 2]

        x = np.arange(len(densities) * len(time_points))
        width = 0.35

        unweighted_scores = []
        weighted_scores = []
        x_labels = []

        for density in densities:
            for time_point in time_points:
                setting_key = f"{density}_{time_point}"

                # æ”¶é›†3Ã—3å’Œ5Ã—5çš„ç»“æœ
                all_unweighted = []
                all_weighted = []

                for patch_size in [3, 5]:
                    patch_key = f"patch{patch_size}"
                    if (patch_key in aggregated_results and
                        setting_key in aggregated_results[patch_key]):

                        # æ— æƒé‡ç»“æœ
                        if aggregated_results[patch_key][setting_key]['unweighted']:
                            unweighted_results = aggregated_results[patch_key][setting_key]['unweighted']
                            avg_score = np.mean([r[metric] for r in unweighted_results])
                            all_unweighted.append(avg_score)

                        # æœ‰æƒé‡ç»“æœ
                        if aggregated_results[patch_key][setting_key]['weighted']:
                            weighted_results = aggregated_results[patch_key][setting_key]['weighted']
                            avg_score = np.mean([r[metric] for r in weighted_results])
                            all_weighted.append(avg_score)

                # è·¨patchå¤§å°çš„å¹³å‡
                unweighted_scores.append(np.mean(all_unweighted) if all_unweighted else 0)
                weighted_scores.append(np.mean(all_weighted) if all_weighted else 0)
                x_labels.append(f"{density}_{time_point}")

        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        bars1 = ax.bar(x - width/2, unweighted_scores, width, label='Unweighted', alpha=0.8)
        bars2 = ax.bar(x + width/2, weighted_scores, width, label='Weighted', alpha=0.8)

        ax.set_xlabel('Setting (Density_Time)', fontsize=10)
        ax.set_ylabel(label, fontsize=10)
        ax.set_title(f'{label} Comparison', fontsize=12, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(x_labels, rotation=45, ha='right', fontsize=8)
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)

        # æ·»åŠ æ”¹å–„ç™¾åˆ†æ¯”
        for i, (uw, w) in enumerate(zip(unweighted_scores, weighted_scores)):
            if uw > 0:
                improvement = ((w - uw) / uw) * 100
                if improvement > 0:
                    ax.text(i, max(uw, w) + 0.01, f'+{improvement:.1f}%',
                            ha='center', va='bottom', fontsize=7, color='green')
                else:
                    ax.text(i, max(uw, w) + 0.01, f'{improvement:.1f}%',
                            ha='center', va='bottom', fontsize=7, color='red')

    plt.tight_layout(rect=[0, 0.05, 1, 0.95])

    return fig

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ å¼€å§‹åˆ›å»ºæ··æ·†çŸ©é˜µå¯è§†åŒ–...")

    # åŠ è½½ç»“æœ
    results = load_results()
    if not results:
        return

    # èšåˆç»“æœ
    aggregated = aggregate_results_by_setting(results)

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    for patch_size, patch_data in aggregated.items():
        print(f"  {patch_size}: {len(patch_data)} settings")

    # åˆ›å»º4å¼ æ··æ·†çŸ©é˜µå›¾
    figures = {}

    for patch_size in [3, 5]:
        for use_weight in [False, True]:
            print(f"ğŸ¨ åˆ›å»ºå›¾: {patch_size}Ã—{patch_size} {'Weighted' if use_weight else 'Unweighted'}")

            fig = create_confusion_matrix_plot(aggregated, patch_size, use_weight)
            filename = f"confusion_matrix_{patch_size}x{patch_size}_{'weighted' if use_weight else 'unweighted'}.png"
            fig.savefig(filename, dpi=300, bbox_inches='tight')
            figures[filename] = fig

            print(f"  âœ… ä¿å­˜åˆ°: {filename}")

    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡å›¾
    print(f"ğŸ“ˆ åˆ›å»ºæ±‡æ€»ç»Ÿè®¡å›¾...")
    summary_fig = create_summary_statistics(aggregated)
    summary_filename = "performance_comparison_summary.png"
    summary_fig.savefig(summary_filename, dpi=300, bbox_inches='tight')
    figures[summary_filename] = summary_fig
    print(f"  âœ… ä¿å­˜åˆ°: {summary_filename}")

    # æ˜¾ç¤ºæ‰€æœ‰å›¾
    print(f"\nğŸ–¼ï¸ æ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„å›¾...")

    # å…³é—­æ‰€æœ‰å›¾ä»¥èŠ‚çœå†…å­˜
    for fig in figures.values():
        plt.close(fig)

    print(f"\nâœ… å¯è§†åŒ–å®Œæˆ!")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for filename in figures.keys():
        print(f"  - {filename}")

    print(f"\nğŸ“‹ å›¾åƒè¯´æ˜:")
    print(f"  - confusion_matrix_3x3_unweighted.png: 3Ã—3 patch æ— æƒé‡æ··æ·†çŸ©é˜µ")
    print(f"  - confusion_matrix_3x3_weighted.png: 3Ã—3 patch æœ‰æƒé‡æ··æ·†çŸ©é˜µ")
    print(f"  - confusion_matrix_5x5_unweighted.png: 5Ã—5 patch æ— æƒé‡æ··æ·†çŸ©é˜µ")
    print(f"  - confusion_matrix_5x5_weighted.png: 5Ã—5 patch æœ‰æƒé‡æ··æ·†çŸ©é˜µ")
    print(f"  - performance_comparison_summary.png: æ€§èƒ½å¯¹æ¯”æ±‡æ€»")

if __name__ == "__main__":
    main()